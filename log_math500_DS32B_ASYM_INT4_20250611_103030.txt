[W611 10:30:32.327516095 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477
       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())
[2025-06-11 10:30:36,129] [[32m    INFO[0m]: PyTorch version 2.6.0+xpu available. (config.py:54)[0m
[W611 10:30:36.471471891 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477
       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())
INFO 06-11 10:30:37 [__init__.py:239] Automatically detected platform xpu.
[2025-06-11 10:30:38,040] [[32m    INFO[0m]: --- LOADING MODEL --- (pipeline.py:187)[0m
[2025-06-11 10:30:38,497] [[32m    INFO[0m]: Triton not installed or not compatible; certain GPU-related functions will not be available. (importing.py:16)[0m
[2025-06-11 10:30:38,623] [[33m WARNING[0m]: Casting torch.bfloat16 to torch.float16. (_logger.py:68)[0m
[2025-06-11 10:30:38,624] [[32m    INFO[0m]: This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'. (config.py:604)[0m
[2025-06-11 10:30:38,624] [[33m WARNING[0m]: The model has a long context length (35585). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value. (_logger.py:68)[0m
[2025-06-11 10:30:38,625] [[32m    INFO[0m]: Disabled the custom all-reduce kernel because it is not supported on current platform. (config.py:1639)[0m
[2025-06-11 10:30:38,627] [[32m    INFO[0m]: Initializing a V0 LLM engine (v0.8.3+ipexllm) with config: model='/llm/intelmc8/models/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='/llm/intelmc8/models/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=True, dtype=torch.float16, max_seq_len=35585, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=/llm/intelmc8/models/DeepSeek-R1-Distill-Qwen-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False,  (llm_engine.py:242)[0m
[2025-06-11 10:30:39,030] [[33m WARNING[0m]: No existing RAY instance detected. A new instance will be launched with current node resources. (_logger.py:68)[0m
[2025-06-11 10:30:40,233] [[32m    INFO[0m]: Started a local Ray instance. (worker.py:1888)[0m
[2025-06-11 10:30:41,208] [[32m    INFO[0m]: No current placement group found. Creating a new placement group. (ray_utils.py:339)[0m
[2025-06-11 10:30:41,382] [[32m    INFO[0m]: use_ray_spmd_worker: False (ray_distributed_executor.py:178)[0m
[36m(pid=18647)[0m [W611 10:30:43.931037386 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.
[36m(pid=18647)[0m   Overriding a previously registered kernel for the same operator and the same dispatch key
[36m(pid=18647)[0m   operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()
[36m(pid=18647)[0m     registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
[36m(pid=18647)[0m   dispatch key: XPU
[36m(pid=18647)[0m   previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477
[36m(pid=18647)[0m        new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())
[36m(pid=18644)[0m INFO 06-11 10:30:46 [__init__.py:239] Automatically detected platform xpu.
[36m(WrapperWithLoadBit pid=18644)[0m INFO 06-11 10:30:47 [importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.
[2025-06-11 10:30:48,636] [[32m    INFO[0m]: non_carry_over_env_vars from config: set() (ray_distributed_executor.py:354)[0m
[2025-06-11 10:30:48,637] [[32m    INFO[0m]: Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_RPC_TIMEOUT', 'VLLM_USE_V1', 'IPEX_LLM_LOWBIT'] (ray_distributed_executor.py:356)[0m
[2025-06-11 10:30:48,637] [[32m    INFO[0m]: If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file (ray_distributed_executor.py:359)[0m
[2025-06-11 10:30:48,667] [[32m    INFO[0m]: Cannot use None backend on XPU. (xpu.py:39)[0m
[2025-06-11 10:30:48,667] [[32m    INFO[0m]: Using IPEX attention backend. (xpu.py:45)[0m
[36m(WrapperWithLoadBit pid=18675)[0m INFO 06-11 10:30:48 [xpu.py:39] Cannot use None backend on XPU.
[36m(WrapperWithLoadBit pid=18675)[0m INFO 06-11 10:30:48 [xpu.py:45] Using IPEX attention backend.
[2025-06-11 10:30:49,437] [[32m    INFO[0m]: vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9eb90c2d'), local_subscribe_addr='ipc:///tmp/f32cd368-2dce-46ff-9d8b-ca1a8276df71', remote_subscribe_addr=None, remote_addr_ipv6=False) (shm_broadcast.py:264)[0m
[2025-06-11 10:30:49,444] [[32m    INFO[0m]: rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0 (parallel_state.py:957)[0m
[2025-06-11 10:30:49,448] [[32m    INFO[0m]: cudagraph sizes specified by model runner [] is overridden by config [] (config.py:3339)[0m
[36m(WrapperWithLoadBit pid=18675)[0m INFO 06-11 10:30:49 [parallel_state.py:957] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2
[36m(WrapperWithLoadBit pid=18675)[0m INFO 06-11 10:30:49 [config.py:3339] cudagraph sizes specified by model runner [] is overridden by config []
Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:04,  1.69it/s]
Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:03,  1.59it/s]
Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:03,  1.64it/s]
Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:02,  1.62it/s]
Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:03<00:01,  1.60it/s]
Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:03<00:01,  1.57it/s]
Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:04<00:00,  1.59it/s]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:04<00:00,  1.98it/s]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:04<00:00,  1.73it/s]

[2025-06-11 10:30:54,516] [[32m    INFO[0m]: Loading weights took 4.64 seconds (loader.py:447)[0m
[2025-06-11 10:30:54,522] [[32m    INFO[0m]: Converting the current model to asym_int4 format...... (convert.py:1112)[0m
[2025-06-11 10:30:54,523] [[32m    INFO[0m]: Only HuggingFace Transformers models are currently supported for further optimizations (convert.py:904)[0m
[2025-06-11 10:31:03,129] [[32m    INFO[0m]: Only HuggingFace Transformers models are currently supported for further optimizations (convert.py:1312)[0m
[36m(WrapperWithLoadBit pid=18675)[0m 2025-06-11 10:31:05,387 - ipex_llm.transformers.utils - INFO - Converting the current model to asym_int4 format......
[36m(WrapperWithLoadBit pid=18675)[0m 2025-06-11 10:31:05,388 - ipex_llm.transformers.utils - INFO - Only HuggingFace Transformers models are currently supported for further optimizations
[36m(pid=18645)[0m [W611 10:30:43.980123632 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=18645)[0m   Overriding a previously registered kernel for the same operator and the same dispatch key[32m [repeated 3x across cluster][0m
[36m(pid=18645)[0m   operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()[32m [repeated 3x across cluster][0m
[36m(pid=18645)[0m     registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6[32m [repeated 3x across cluster][0m
[36m(pid=18645)[0m   dispatch key: XPU[32m [repeated 3x across cluster][0m
[36m(pid=18645)[0m   previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477[32m [repeated 3x across cluster][0m
[36m(pid=18645)[0m        new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())[32m [repeated 3x across cluster][0m
[36m(WrapperWithLoadBit pid=18675)[0m INFO 06-11 10:31:05 [loader.py:447] Loading weights took 15.58 seconds
[36m(pid=18645)[0m INFO 06-11 10:30:47 [__init__.py:239] Automatically detected platform xpu.[32m [repeated 3x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m INFO 06-11 10:30:48 [importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.[32m [repeated 3x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m INFO 06-11 10:30:48 [xpu.py:39] Cannot use None backend on XPU.[32m [repeated 2x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m INFO 06-11 10:30:48 [xpu.py:45] Using IPEX attention backend.[32m [repeated 2x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m INFO 06-11 10:30:49 [parallel_state.py:957] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3[32m [repeated 2x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m INFO 06-11 10:30:49 [config.py:3339] cudagraph sizes specified by model runner [] is overridden by config [][32m [repeated 2x across cluster][0m
[2025-06-11 10:31:06,483] [[32m    INFO[0m]: Loading model weights took 4.5868 GB (model_convert.py:160)[0m
[36m(WrapperWithLoadBit pid=18647)[0m 2025-06-11 10:31:06,151 - ipex_llm.transformers.utils - INFO - Converting the current model to asym_int4 format......[32m [repeated 2x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m 2025-06-11 10:31:30,719 - ipex_llm.transformers.utils - INFO - Only HuggingFace Transformers models are currently supported for further optimizations[32m [repeated 3x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m 2025-06-11 10:31:33,492 - ipex_llm.vllm.xpu.model_convert - INFO - Loading model weights took 4.5868 GB
[36m(WrapperWithLoadBit pid=18647)[0m 2025-06-11 10:31:36,477 - ipex_llm.transformers.utils - INFO - Only HuggingFace Transformers models are currently supported for further optimizations[32m [repeated 2x across cluster][0m
[36m(WrapperWithLoadBit pid=18647)[0m 2025-06-11 10:31:39,499 - ipex_llm.vllm.xpu.model_convert - INFO - Loading model weights took 4.5868 GB[32m [repeated 2x across cluster][0m
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_WORKER_COUNT changed to be 2 (default:1)
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_ATL_SHM changed to be 1 (default:0)
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_DG2_ALLREDUCE changed to be 1 (default:0)
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_LOCAL_RANK changed to be 0 (default:-1)
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_LOCAL_SIZE changed to be 4 (default:-1)
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be none (default:hydra)
2025:06:11-10:31:40:(17963) |CCL_WARN| value of CCL_ZE_IPC_EXCHANGE changed to be sockets (default:pidfd)
2025:06:11-10:31:41:(17963) |CCL_WARN| device_family is unknown, topology discovery could be incorrect, it might result in suboptimal performance
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:06:11-10:31:41:(17963) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
-----> current rank: 0, world size: 4, byte_count: 364390400,is_p2p:1
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| device_family is unknown, topology discovery could be incorrect, it might result in suboptimal performance
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m 2025:06:11-10:31:41:(18675) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
[36m(WrapperWithLoadBit pid=18675)[0m -----> current rank: 2, world size: 4, byte_count: 364390400,is_p2p:1
[36m(WrapperWithLoadBit pid=18647)[0m INFO 06-11 10:31:06 [loader.py:447] Loading weights took 16.27 seconds[32m [repeated 2x across cluster][0m
[2025-06-11 10:31:56,879] [[33m WARNING[0m]: Pin memory is not supported on XPU. (_logger.py:68)[0m
[2025-06-11 10:31:57,592] [[32m    INFO[0m]: # xpu blocks: 10032, # CPU blocks: 8192 (executor_base.py:112)[0m
[2025-06-11 10:31:57,592] [[32m    INFO[0m]: Maximum concurrency for 35585 tokens per request: 2.26x (executor_base.py:117)[0m
[36m(WrapperWithLoadBit pid=18675)[0m WARNING 06-11 10:31:57 [_logger.py:68] Pin memory is not supported on XPU.
[36m(WrapperWithLoadBit pid=18645)[0m 2025:06:11-10:31:41:(18645) |CCL_WARN| device_family is unknown, topology discovery could be incorrect, it might result in suboptimal performance[32m [repeated 2x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m 2025:06:11-10:31:41:(18645) |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices[32m [repeated 24x across cluster][0m
[36m(WrapperWithLoadBit pid=18645)[0m -----> current rank: 3, world size: 4, byte_count: 364390400,is_p2p:1[32m [repeated 2x across cluster][0m
[2025-06-11 10:31:59,839] [[32m    INFO[0m]: init engine (profile, create kv cache, warmup model) took 20.34 seconds (llm_engine.py:448)[0m
[2025-06-11 10:31:59,906] [[32m    INFO[0m]: --- INIT SEEDS --- (pipeline.py:258)[0m
[2025-06-11 10:31:59,907] [[32m    INFO[0m]: --- LOADING TASKS --- (pipeline.py:212)[0m
[2025-06-11 10:31:59,907] [[33m WARNING[0m]: If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`. (_logger.py:68)[0m
[2025-06-11 10:31:59,910] [[32m    INFO[0m]: HuggingFaceH4/MATH-500 default (lighteval_task.py:187)[0m
[2025-06-11 10:31:59,910] [[33m WARNING[0m]: Careful, the task lighteval|math_500 is using evaluation data to build the few shot examples. (_logger.py:68)[0m
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 500 examples [00:00, 30260.19 examples/s]
[2025-06-11 10:32:04,241] [[32m    INFO[0m]: --- RUNNING MODEL --- (pipeline.py:462)[0m
[2025-06-11 10:32:04,241] [[32m    INFO[0m]: Running RequestType.GREEDY_UNTIL requests (pipeline.py:466)[0m
[2025-06-11 10:32:04,374] [[33m WARNING[0m]: You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring. (_logger.py:68)[0m
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/2000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   0%|          | 4/2000 [04:21<36:12:27, 65.30s/it, est. speed input: 13.29 toks/s, output: 38.45 toks/s][A
Processed prompts:   0%|          | 8/2000 [10:46<46:13:57, 83.55s/it, est. speed input: 10.18 toks/s, output: 42.74 toks/s][A
Processed prompts:   1%|          | 12/2000 [21:28<65:31:24, 118.65s/it, est. speed input: 7.42 toks/s, output: 41.32 toks/s][A
Processed prompts:   1%|          | 16/2000 [28:24<62:12:46, 112.89s/it, est. speed input: 6.87 toks/s, output: 44.34 toks/s][A
Processed prompts:   1%|          | 20/2000 [36:44<64:28:36, 117.23s/it, est. speed input: 6.08 toks/s, output: 42.87 toks/s][A
Processed prompts:   1%|          | 24/2000 [38:20<46:58:08, 85.57s/it, est. speed input: 6.55 toks/s, output: 44.19 toks/s] [A
Processed prompts:   1%|â–         | 28/2000 [55:28<77:33:31, 141.59s/it, est. speed input: 4.95 toks/s, output: 41.58 toks/s][A
Processed prompts:   2%|â–         | 32/2000 [1:08:54<87:51:24, 160.71s/it, est. speed input: 4.33 toks/s, output: 41.97 toks/s][A
Processed prompts:   2%|â–         | 36/2000 [1:10:33<64:29:22, 118.21s/it, est. speed input: 4.62 toks/s, output: 42.74 toks/s][A
Processed prompts:   2%|â–         | 40/2000 [1:29:55<93:18:19, 171.38s/it, est. speed input: 3.87 toks/s, output: 40.74 toks/s][A
Processed prompts:   2%|â–         | 44/2000 [1:37:27<83:24:29, 153.51s/it, est. speed input: 3.78 toks/s, output: 41.89 toks/s][A
Processed prompts:   2%|â–         | 48/2000 [1:45:34<78:00:24, 143.86s/it, est. speed input: 3.70 toks/s, output: 41.13 toks/s][A
Processed prompts:   3%|â–Ž         | 52/2000 [1:45:47<54:47:43, 101.26s/it, est. speed input: 3.85 toks/s, output: 41.83 toks/s][A
Processed prompts:   3%|â–Ž         | 56/2000 [1:51:58<53:16:47, 98.67s/it, est. speed input: 3.78 toks/s, output: 41.66 toks/s] [A
Processed prompts:   3%|â–Ž         | 60/2000 [1:53:03<39:46:19, 73.80s/it, est. speed input: 3.97 toks/s, output: 41.77 toks/s][A
Processed prompts:   3%|â–Ž         | 64/2000 [1:57:34<38:42:58, 71.99s/it, est. speed input: 4.00 toks/s, output: 41.83 toks/s][A
Processed prompts:   3%|â–Ž         | 68/2000 [2:01:29<36:29:54, 68.01s/it, est. speed input: 4.03 toks/s, output: 41.54 toks/s][A
Processed prompts:   4%|â–Ž         | 72/2000 [2:10:39<47:37:40, 88.93s/it, est. speed input: 3.86 toks/s, output: 39.15 toks/s][A
Processed prompts:   4%|â–         | 76/2000 [2:10:50<33:41:33, 63.04s/it, est. speed input: 4.02 toks/s, output: 41.75 toks/s][A
Processed prompts:   4%|â–         | 80/2000 [2:19:10<43:32:23, 81.64s/it, est. speed input: 3.89 toks/s, output: 41.50 toks/s][A
Processed prompts:   4%|â–         | 84/2000 [2:34:39<67:30:56, 126.86s/it, est. speed input: 3.59 toks/s, output: 41.04 toks/s][A
Processed prompts:   4%|â–         | 88/2000 [2:36:03<50:28:26, 95.03s/it, est. speed input: 3.69 toks/s, output: 41.50 toks/s] [A
Processed prompts:   5%|â–         | 92/2000 [2:47:53<63:29:13, 119.79s/it, est. speed input: 3.54 toks/s, output: 40.95 toks/s][A
Processed prompts:   5%|â–         | 96/2000 [2:54:09<59:16:15, 112.07s/it, est. speed input: 3.50 toks/s, output: 41.20 toks/s][A
Processed prompts:   5%|â–Œ         | 100/2000 [2:55:12<43:52:48, 83.14s/it, est. speed input: 3.60 toks/s, output: 41.42 toks/s][A